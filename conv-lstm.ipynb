{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (conv1): Conv2d(1, 2, kernel_size=(3, 100), stride=(1, 1))\n",
      "  (lstm1): LSTM(402, 300, dropout=0.5)\n",
      "  (hidden2label1): Linear(in_features=300, out_features=5, bias=True)\n",
      "  (batchnorm): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt  \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import datetime\n",
    "import torch.utils.data as Data \n",
    "torch.manual_seed(5)\n",
    "\n",
    "#参数\n",
    "\n",
    "D = 300 \n",
    "C = 5\n",
    "Ci=1\n",
    "len_sentence=57\n",
    "dropout_rate=0.5 \n",
    "kernel_size_x1=3\n",
    "kernel_size_y1=4 \n",
    "kernel_size1=(kernel_size_x1,kernel_size_y1) \n",
    "batch_size1=len_sentence-kernel_size_x1+1 \n",
    "Kys=100\n",
    "Co=2\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        # print(args)\n",
    "        self.hidden_dim =300 #(300-Kys+1)*Co\n",
    "        self.num_layers =1\n",
    "\n",
    "\n",
    "\n",
    "        '''\n",
    "        #BiLSTM\n",
    "        self.bilstm = nn.LSTM(D_lstm, self.hidden_dim, num_layers=self.num_layers, dropout=dropout_rate, bidirectional=True, bias=True)\n",
    "        print(self.bilstm)\n",
    "        self.hidden2label = nn.Linear(self.hidden_dim * 2, C)\n",
    "        '''\n",
    "\n",
    "        self.conv1 = nn.Conv2d(Ci, Co, (kernel_size_x1, Kys))\n",
    "\n",
    "        self.lstm1 = nn.LSTM((300-Kys+1)*Co, self.hidden_dim, dropout=dropout_rate, num_layers=self.num_layers)\n",
    "        # linear\n",
    "        self.hidden2label1 = nn.Linear(self.hidden_dim, C)\n",
    "        # hidden\n",
    "        self.hidden1 = self.init_hidden(self.num_layers, batch_size1)\n",
    "        # dropout\n",
    "        self.batchnorm=nn.BatchNorm2d(1)\n",
    "\n",
    "    def init_hidden(self, num_layers, batch_size):\n",
    "        # the first is the hidden h\n",
    "        # the second is the cell  c\n",
    "         return (Variable(torch.zeros(1*num_layers, batch_size, self.hidden_dim)).cuda(),\n",
    "                  Variable(torch.zeros(1*num_layers, batch_size, self.hidden_dim)).cuda())\n",
    "        #return (Variable(torch.randn(1 * num_layers, batch_size, self.hidden_dim)),\n",
    "        #         Variable(torch.randn(1 * num_layers, batch_size, self.hidden_dim)))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1) # (N,Ci,W,D)\n",
    "        #print x.size()\n",
    "        \n",
    "        x1=self.conv1(x)#remove pooling\n",
    "        y1=x1[:,0]\n",
    "\n",
    "        for i in range(x1.size()[1]-1):\n",
    "            y1=torch.cat((y1,x1[:,i+1]),2)\n",
    "        x1=y1\n",
    "        x1 = x1.view(len(x1), x1.size(1), -1)\n",
    "        \n",
    "        x1 = x1.unsqueeze(1)\n",
    "        x1=self.batchnorm(x1)\n",
    "        x1 = x1.squeeze(1)\n",
    "\n",
    "\n",
    "        lstm_out1, self.hidden1 = self.lstm1(x1, self.hidden1)\n",
    "\n",
    "        \n",
    "        lstm_out1 = torch.transpose(lstm_out1, 1, 2)\n",
    "\n",
    "        # pooling\n",
    "        lstm_out1 = F.max_pool1d(lstm_out1, lstm_out1.size(2)).squeeze(2)\n",
    "\n",
    "        lstm_out1 = F.tanh(lstm_out1)\n",
    "        # linear\n",
    "        y1 = self.hidden2label1(lstm_out1)\n",
    "        logit=y1\n",
    "        return logit\n",
    "\n",
    "net = LSTM() \n",
    "net=net.cuda()\n",
    "print(net)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
